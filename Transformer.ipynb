{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6915,"status":"ok","timestamp":1724306819105,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"_PA5sdzSqExF","outputId":"cdb5e543-4058-41a3-a7f5-358cd4c4ce2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting silence_tensorflow\n","  Downloading silence_tensorflow-1.2.2.tar.gz (5.3 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Building wheels for collected packages: silence_tensorflow\n","  Building wheel for silence_tensorflow (setup.py): started\n","  Building wheel for silence_tensorflow (setup.py): finished with status 'done'\n","  Created wheel for silence_tensorflow: filename=silence_tensorflow-1.2.2-py3-none-any.whl size=5859 sha256=1c8072e86b6250d9897ea56cdae2fdeac35b37e8a65a0542dda77723069f907c\n","  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\b9\\67\\62\\dff7e2461cfde5bc7ef494df132939898fc704128a32ca37ee\n","Successfully built silence_tensorflow\n","Installing collected packages: silence_tensorflow\n","Successfully installed silence_tensorflow-1.2.2\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.1.2 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"ename":"TypeError","evalue":"Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\__init__.py:46\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Ensure TensorFlow is importable and its version is sufficiently recent. This\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# needs to happen before anything else, since the imports below will try to\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# import tensorflow, too.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_compat\n\u001b[0;32m     47\u001b[0m tf_compat\u001b[38;5;241m.\u001b[39mensure_tf_install()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Imports for registration\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\__init__.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_compat\n\u001b[0;32m     21\u001b[0m tf_compat\u001b[38;5;241m.\u001b[39mensure_tf_install()\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeamBasedBuilder  \u001b[38;5;66;03m# pylint:disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BuilderConfig\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetBuilder\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m naming\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registered\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m splits \u001b[38;5;28;01mas\u001b[39;00m splits_lib\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tfrecords_reader\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tfrecords_writer\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\splits.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoves\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mrange\u001b[39m  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m proto\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     33\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mas_proto_cls(proto\u001b[38;5;241m.\u001b[39mSplitInfo)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSplitInfo\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\proto\\__init__.py:27\u001b[0m\n\u001b[0;32m     23\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_format \u001b[38;5;28;01mas\u001b[39;00m json_format_\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dataset_info_pb2_, SplitInfo_, json_format_\n\u001b[1;32m---> 27\u001b[0m dataset_info_pb2, SplitInfo, json_format \u001b[38;5;241m=\u001b[39m \u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _get\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\proto\\__init__.py:21\u001b[0m, in \u001b[0;36m_get\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m():\n\u001b[1;32m---> 21\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_info_generated_pb2 \u001b[38;5;28;01mas\u001b[39;00m dataset_info_pb2_\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_info_generated_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SplitInfo \u001b[38;5;28;01mas\u001b[39;00m SplitInfo_\n\u001b[0;32m     23\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_format \u001b[38;5;28;01mas\u001b[39;00m json_format_\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\proto\\dataset_info_generated_pb2.py:55\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_metadata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv0\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m schema_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow__metadata_dot_proto_dot_v0_dot_schema__pb2\n\u001b[0;32m     36\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     37\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_info.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     38\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow_datasets\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m   ,\n\u001b[0;32m     43\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[tensorflow__metadata_dot_proto_dot_v0_dot_statistics__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,tensorflow__metadata_dot_proto_dot_v0_dot_schema__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n\u001b[0;32m     48\u001b[0m _DATASETLOCATION \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     49\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatasetLocation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     50\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow_datasets.DatasetLocation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     51\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     53\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 55\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow_datasets.DatasetLocation.urls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     62\u001b[0m   ],\n\u001b[0;32m     63\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     64\u001b[0m   ],\n\u001b[0;32m     65\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     66\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     67\u001b[0m   ],\n\u001b[0;32m     68\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     69\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     70\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     71\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     72\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     73\u001b[0m   ],\n\u001b[0;32m     74\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m133\u001b[39m,\n\u001b[0;32m     75\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m164\u001b[39m,\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     79\u001b[0m _SPLITINFO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     80\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSplitInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     81\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow_datasets.SplitInfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m305\u001b[39m,\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    131\u001b[0m _SUPERVISEDKEYS \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m    132\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupervisedKeys\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    133\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow_datasets.SupervisedKeys\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m354\u001b[39m,\n\u001b[0;32m    166\u001b[0m )\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\descriptor.py:553\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[0;32m    548\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[0;32m    549\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    551\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    552\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n","\u001b[1;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"]}],"source":["import os\n","! pip install silence_tensorflow\n","from silence_tensorflow import silence_tensorflow\n","import logging\n","logging.getLogger('tensorflow').disabled = True\n","import re\n","import numpy as np\n","import pandas as pd\n","from time import time\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import os\n","from PyPDF2 import PdfReader\n","import pdfplumber\n","import re\n","import csv"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":442,"status":"ok","timestamp":1724306358077,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"8KgYn35Aqu6X"},"outputs":[],"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":445,"status":"ok","timestamp":1724306375253,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"g6v98A1Hqpqh"},"outputs":[],"source":["# Maximum sentence length\n","MAX_LENGTH = 20\n","# For tf.data.Dataset\n","BATCH_SIZE = int(64 * strategy.num_replicas_in_sync)\n","# BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","# For Transformer\n","NUM_LAYERS = 6 #6\n","D_MODEL = 512 #512\n","NUM_HEADS = 8\n","UNITS = 2048 #2048\n","DROPOUT = 0.5\n","EPOCHS = 500"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1724306390839,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"s7hbmdezq3Lf"},"outputs":[],"source":["def textPreprocess(input_text):\n","\n","  def removeAccents(input_text):\n","      strange='ąćęłńóśżź'\n","      ascii_replacements='acelnoszz'\n","      translator=str.maketrans(strange,ascii_replacements)\n","      return input_text.translate(translator)\n","\n","  def removeSpecial(input_text):\n","      special='[^A-Za-z0-9 ]+'\n","      return re.sub(special, '', input_text)\n","\n","  def removeTriplicated(input_text):\n","      return re.compile(r'(.)\\1{2,}', re.IGNORECASE).sub(r'\\1', input_text)\n","\n","  return removeTriplicated(removeSpecial(removeAccents(input_text.lower())))\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":420,"status":"ok","timestamp":1724306410999,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"iD3d4GbZq6la"},"outputs":[],"source":["def tokenize_and_filter(inputs, outputs,START_TOKEN ,END_TOKEN,tokenizer):\n","  tokenized_inputs, tokenized_outputs = [], []\n","\n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # tokenize sentence\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","    # check tokenized sentence max length\n","    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n","      tokenized_inputs.append(sentence1)\n","      tokenized_outputs.append(sentence2)\n","\n","  # pad tokenized sentences\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","\n","  return tokenized_inputs, tokenized_outputs"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":402,"status":"ok","timestamp":1724306428940,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"um3qjTPDrAao"},"outputs":[],"source":["def scaled_dot_product_attention(query, key, value, mask):\n","  \"\"\"Calculate the attention weights. \"\"\"\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # scale matmul_qk\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # softmax is normalized on the last axis (seq_len_k)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":428,"status":"ok","timestamp":1724306511579,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"G-C9Qj0crDl0"},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    self.depth = d_model // self.num_heads\n","\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # linear layers\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # split heads\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # scaled dot-product attention\n","    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # concatenation of heads\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # final linear layer\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":518,"status":"ok","timestamp":1724306522106,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"aw4iXdlarV8-"},"outputs":[],"source":["def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, sequence length)\n","  return mask[:, tf.newaxis, tf.newaxis, :]"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1724306528469,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"-CAxePscrYYt"},"outputs":[],"source":["def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x)\n","  return tf.maximum(look_ahead_mask, padding_mask)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":459,"status":"ok","timestamp":1724306545210,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"_vI_RPZnrcrZ"},"outputs":[],"source":["class PositionalEncoding(tf.keras.layers.Layer):\n","\n","  def __init__(self, position, d_model):\n","    super(PositionalEncoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","  def get_angles(self, position, i, d_model):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position, d_model):\n","    angle_rads = self.get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","        d_model=d_model)\n","    # apply sin to even index in the array\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","    # apply cos to odd index in the array\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = tf.concat([sines, cosines], axis=-1)\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, inputs):\n","        output = inputs + self.pos_encoding[:, : tf.shape(inputs)[1]]\n","        return tf.cast(output, dtype=tf.float32)\n","\n","  def compute_output_shape(self, input_shape):\n","        batch_size, seq_len, d_model = input_shape\n","        return (batch_size, seq_len, d_model)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":428,"status":"ok","timestamp":1724306557141,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"k9XI2Pzerf4E"},"outputs":[],"source":["\n","def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs,\n","          'key': inputs,\n","          'value': inputs,\n","          'mask': padding_mask\n","      })\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":688,"status":"ok","timestamp":1724306571666,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"WblAFlY2rjaY"},"outputs":[],"source":["def encoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","  for i in range(int(num_layers)):\n","    outputs = encoder_layer(\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724306589197,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"bPmzMM9-rnbZ"},"outputs":[],"source":["def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs,\n","          'key': inputs,\n","          'value': inputs,\n","          'mask': look_ahead_mask\n","      })\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1,\n","          'key': enc_outputs,\n","          'value': enc_outputs,\n","          'mask': padding_mask\n","      })\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":520,"status":"ok","timestamp":1724306603772,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"72TfNlCXrrHv"},"outputs":[],"source":["\n","def decoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  for i in range(int(num_layers)):\n","    outputs = decoder_layer(\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":400,"status":"ok","timestamp":1724306620053,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"drweee9prusO"},"outputs":[],"source":["def transformer(vocab_size,\n","                num_layers,\n","                units,\n","                d_model,\n","                num_heads,\n","                dropout,\n","                name=\"transformer\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","  # mask the future tokens for decoder inputs at the 1st attention block\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask,\n","      output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","  # mask the encoder outputs for the 2nd attention block\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  enc_outputs = encoder(\n","      vocab_size=vocab_size,\n","      num_layers=num_layers,\n","      units=units,\n","      d_model=d_model,\n","      num_heads=num_heads,\n","      dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask])\n","\n","  dec_outputs = decoder(\n","      vocab_size=vocab_size,\n","      num_layers=num_layers,\n","      units=units,\n","      d_model=d_model,\n","      num_heads=num_heads,\n","      dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":447,"status":"ok","timestamp":1724306631951,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"K7E9t-0tryI1"},"outputs":[],"source":["def match_seq_len(y_true , y_pred):\n","   seq_len_pred = tf.shape(y_pred)[1]\n","   seq_len_true = tf.shape(y_true)[1]\n","   pad_size = tf.maximum(0, seq_len_pred - seq_len_true)\n","   y_true_padded = tf.pad(y_true, [[0, 0], [0, pad_size]], constant_values=0)\n","   y_true_truncated = y_true[:, :seq_len_pred]\n","   y_true_adjusted = tf.cond(seq_len_true < seq_len_pred,lambda: y_true_padded,lambda: y_true_truncated)\n","\n","   return y_true_adjusted, y_pred"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":429,"status":"ok","timestamp":1724306642223,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"I1os0sP5r0o2"},"outputs":[],"source":["\n","def loss_function(y_true, y_pred):\n","  y_true , y_pred = match_seq_len(y_true , y_pred)\n","  y_true = tf.reshape(y_true, shape=(-1,  tf.shape(y_pred)[1]))\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","  return tf.reduce_sum(loss) / (tf.reduce_sum(mask) + tf.keras.backend.epsilon())\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":695,"status":"ok","timestamp":1724306661629,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"XPhBc_x3r3TF"},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps**-1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","optimizer = tf.keras.optimizers.Adam()\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":514,"status":"ok","timestamp":1724306679403,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"A8hQANTMr8hX"},"outputs":[],"source":["\n","def evaluate(sentence, model):\n","\n","  sentence = tf.expand_dims(\n","      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN, 0)\n","\n","  for i in range(MAX_LENGTH):\n","    predictions = model(inputs=[sentence, output], training=False)\n","\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","\n","    if tf.equal(predicted_id, END_TOKEN[0]):\n","      break\n","\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xMPlhC8sBTU"},"outputs":[],"source":["def predict(sentence,model):\n","  prediction = evaluate(sentence,model)\n","\n","  predicted_sentence = tokenizer.decode(\n","      [i for i in prediction if i < tokenizer.vocab_size])\n","\n","  print('Input: {}'.format(sentence))\n","  print('Output: {}'.format(predicted_sentence))\n","\n","  return predicted_sentence\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":720,"status":"ok","timestamp":1724306849988,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"4uJWcDi6skjH"},"outputs":[],"source":["def extract_text_from_pdf(pdf_path):\n","    reader = PdfReader(pdf_path)\n","    text = \"\"\n","    for page in reader.pages:\n","        text += page.extract_text()\n","    return text"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":602,"status":"ok","timestamp":1724306872312,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"FBgzT2WAsr12"},"outputs":[],"source":["\n","def categorize_text_as_heading_content(text):\n","    # Pattern for headings (all-caps assumed as headings)\n","    heading_pattern = re.compile(r'^[A-Z\\s]{2,}$')\n","\n","    lines = text.splitlines()\n","    categorized_content = []\n","    current_heading = None\n","    current_content = []\n","\n","    for line in lines:\n","        stripped_line = line.strip()\n","        if heading_pattern.match(stripped_line):\n","            if current_heading:\n","                # Finalize the previous heading and its content\n","                categorized_content.append((current_heading, \" \".join(current_content)))\n","                current_content = []\n","            current_heading = stripped_line\n","        else:\n","            current_content.append(stripped_line)\n","\n","    # Add any remaining content under the last heading\n","    if current_heading:\n","        categorized_content.append((current_heading, \" \".join(current_content)))\n","\n","    return categorized_content"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":686,"status":"ok","timestamp":1724306885667,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"uYxnnUgXsvEd"},"outputs":[],"source":["def process_pdf(file_path):\n","    with pdfplumber.open(file_path) as pdf:\n","        full_text = \"\"\n","        for page in pdf.pages:\n","            full_text += page.extract_text() + \"\\n\"\n","\n","        categorized_content = categorize_text_as_heading_content(full_text)\n","        return categorized_content\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1724306895137,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"pX-aPCbEsyEQ"},"outputs":[],"source":["def convert_to_csv(pdf_path,output_file = \"Dataset/CSV_Dataset/output_file.csv\"):\n","    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['question', 'answer'])  # Write the header\n","        content = process_pdf(pdf_path)\n","        for heading, contents in content:\n","            writer.writerow([heading, contents])\n"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1724306973395,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"M5N3Afvhs23x"},"outputs":[],"source":["def get_vocab_size(questions,answers):\n","    tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","        questions + answers, target_vocab_size=2**13)\n","    START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","\n","    # Vocabulary size plus start and end token\n","    VOCAB_SIZE = tokenizer.vocab_size + 2\n","\n","    return START_TOKEN, END_TOKEN , VOCAB_SIZE , tokenizer"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1724306969296,"user":{"displayName":"srinithi s","userId":"16975518212146517203"},"user_tz":-330},"id":"QWPSa4bztCEy"},"outputs":[],"source":["def model_fit(output_file = \"Dataset/CSV_Dataset/output_file.csv\" ): #csv path\n","    df = pd.read_csv(output_file)\n","    df['question'] = df[\"question\"].apply(lambda x: textPreprocess(str(x)))\n","    df['answer'] = df[\"answer\"].apply(lambda x: textPreprocess(str(x)))\n","    questions, answers = df['question'].tolist(), df['answer'].tolist()\n","    START_TOKEN,END_TOKEN,VOCAB_SIZE,tokenizer =get_vocab_size(questions,answers)\n","\n","    questions, answers = tokenize_and_filter(questions, answers,START_TOKEN , END_TOKEN,tokenizer)\n","    print('Vocab size: {}'.format(VOCAB_SIZE))\n","    print('Number of samples: {}'.format(len(questions)))\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((\n","        {\n","            'inputs': questions,\n","            'dec_inputs': answers[:, :-1]\n","        },\n","        {\n","            'outputs': answers[:, 1:]\n","        },\n","    ))\n","    dataset = dataset.cache()\n","    dataset = dataset.shuffle(BUFFER_SIZE)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","\n","    tf.keras.backend.clear_session()\n","    # learning_rate = CustomSchedule(D_MODEL)\n","    # optimizer = tf.keras.optimizers.Adam()\n","\n","    with strategy.scope():\n","        model = transformer(\n","            vocab_size=VOCAB_SIZE,\n","            num_layers=NUM_LAYERS,\n","            units=UNITS,\n","            d_model=D_MODEL,\n","            num_heads=NUM_HEADS,\n","            dropout=DROPOUT)\n","\n","        model.compile(optimizer='adam', loss=loss_function)\n","    model.summary()\n","\n","    import datetime\n","    # logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","    # tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","    model.fit(dataset, epochs = 150)\n","    model.save_weights(f'Weights/saved-weights.weights.h5')\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pdf_folder = \"/Dataset/PDF_Dataset\" # Fixed the path here\n","all_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n","\n","if all_files:\n","    print(f\"Found {len(all_files)} PDF files in the dataset folder.\")\n","    for file_name in all_files:\n","            file_path = os.path.join(pdf_folder, file_name)\n","            convert_to_csv(file_path)\n","    print(\"All pdf are converted\")\n","    # Loop through each file and load it into the model\n","    loaded_model = model_fit()\n","    print(\"Model training completed!\")\n","else:\n","    print(\"No PDF files found in the dataset folder.\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOi8A1ChfSUaLkQ5BNguIZa","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
